{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers"
      ],
      "metadata": {
        "id": "GYrdSoU1J1cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMMShC7EmC35"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# End-to-end model profiling with Hugging Face + torchax + JAX\n",
        "# ------------------------------------------------------------\n",
        "import numpy as np\n",
        "\n",
        "import torchax\n",
        "from transformers import (\n",
        "    AutoImageProcessor,\n",
        "    AutoModelForImageClassification,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForCausalLM,\n",
        "    FlaxViTForImageClassification,\n",
        "    FlaxBertForSequenceClassification,\n",
        "    FlaxGPT2LMHeadModel,\n",
        "    FlaxGPTNeoForCausalLM,\n",
        ")\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Optional: Diffusion UNet (requires diffusers)\n",
        "try:\n",
        "    from diffusers import UNet2DConditionModel, FlaxUNet2DConditionModel\n",
        "    HAS_DIFFUSERS = True\n",
        "except ImportError:\n",
        "    HAS_DIFFUSERS = False\n",
        "\n",
        "print(f\"Successfully Installed Diffusers? {HAS_DIFFUSERS}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "YhwVC3lfQ4gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlir-opt"
      ],
      "metadata": {
        "id": "3shl43R7-4An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_key = random.PRNGKey(0)\n",
        "num_runs = 10"
      ],
      "metadata": {
        "id": "GlePQgD3vrMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gemm(a, b):\n",
        "  return jnp.matmul(a, b)"
      ],
      "metadata": {
        "id": "ZoX04oMzmbA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ffn(x, w1, b1, w2, b2):\n",
        "  \"\"\"\n",
        "  Feed Forward Network:\n",
        "    Input: (128, 64, 256)\n",
        "    w1=(256,1024), b1=(1024)\n",
        "    w2=(1024,256), b2=(256)\n",
        "  \"\"\"\n",
        "  h = jnp.matmul(x, w1) + b1\n",
        "  h = jax.nn.relu(h)\n",
        "  return jnp.matmul(h, w2) + b2"
      ],
      "metadata": {
        "id": "DhpeErb_pGkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mha(x, w_q, w_k, w_v, w_o, num_heads=16):\n",
        "    \"\"\"\n",
        "    Multi-Head Self-Attention (MHA) benchmark.\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor of shape (batch, seq_len, embed_dim)\n",
        "        w_q, w_k, w_v: Projection weights for Q, K, V (embed_dim, embed_dim)\n",
        "        w_o: Output projection weight (embed_dim, embed_dim)\n",
        "        num_heads: Number of attention heads (default 16)\n",
        "    Returns:\n",
        "        Tensor of shape (batch, seq_len, embed_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    batch, seq_len, embed_dim = x.shape\n",
        "    head_dim = embed_dim // num_heads\n",
        "\n",
        "    # Get query, key, and value vectors for each token.\n",
        "    q = jnp.dot(x, w_q)\n",
        "    k = jnp.dot(x, w_k)\n",
        "    v = jnp.dot(x, w_v)\n",
        "\n",
        "    # Reshape to (batch, num_heads, seq_len, head_dim).\n",
        "    def reshape_heads(t):\n",
        "        return t.reshape(batch, seq_len, num_heads, head_dim).transpose(0, 2, 1, 3)\n",
        "\n",
        "    q, k, v = map(reshape_heads, (q, k, v))\n",
        "\n",
        "    # Scaled dot product attention per head.\n",
        "    d_k = head_dim\n",
        "    scores = jnp.matmul(q, jnp.swapaxes(k, -2, -1)) / jnp.sqrt(d_k)   # (B, H, S, S)\n",
        "    attn_weights = jax.nn.softmax(scores, axis=-1)\n",
        "    attn_output = jnp.matmul(attn_weights, v)                         # (B, H, S, d_k)\n",
        "\n",
        "    # Recombine heads.\n",
        "    attn_output = attn_output.transpose(0, 2, 1, 3).reshape(batch, seq_len, embed_dim)\n",
        "\n",
        "    # Project back input embedding dimension.\n",
        "    output = jnp.dot(attn_output, w_o)\n",
        "    return output"
      ],
      "metadata": {
        "id": "AR7TPv4QoRAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMP\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def mha_pytorch_style(x, w_qkv, b_qkv, w_o, b_o, num_heads=16):\n",
        "    \"\"\"\n",
        "    Architecturally equivalent to PyTorch's nn.MultiheadAttention.\n",
        "\n",
        "    Args:\n",
        "        x: Input (Batch, Seq, Embed)\n",
        "           Note: If mimicking batch_first=False, transpose x before calling.\n",
        "        w_qkv: Fused Projection Weights (Embed, 3 * Embed)\n",
        "        b_qkv: Fused Projection Bias (3 * Embed)\n",
        "        w_o: Output Projection Weight (Embed, Embed)\n",
        "        b_o: Output Projection Bias (Embed)\n",
        "    \"\"\"\n",
        "    batch, seq_len, embed_dim = x.shape\n",
        "    head_dim = embed_dim // num_heads\n",
        "\n",
        "    # 1. Linear Projection (Fused) + Explicit Bias Add\n",
        "    # Matches PyTorch IR: dot_general followed by add\n",
        "    # shape: (Batch, Seq, 3 * Embed)\n",
        "    qkv = jnp.dot(x, w_qkv) + b_qkv\n",
        "\n",
        "    # 2. Slice Q, K, V\n",
        "    # Matches PyTorch IR: stablehlo.slice\n",
        "    # PyTorch stores weights as (3*Embed, Embed), JAX usually (Embed, 3*Embed).\n",
        "    # We assume w_qkv is already in JAX layout.\n",
        "    q, k, v = jnp.split(qkv, 3, axis=-1)\n",
        "\n",
        "    # 3. Reshape and Transpose for \"Merged Batch/Heads\" Optimization\n",
        "    # PyTorch collapses (Batch, Heads) into one dim for BMM.\n",
        "    # Target shape: (Batch * Heads, Seq, HeadDim)\n",
        "    # This generates the <2048x64x16> tensors seen in the IR.\n",
        "    def to_3d_layout(t):\n",
        "        # (B, S, E) -> (B, S, H, D) -> (B, H, S, D) -> (B*H, S, D)\n",
        "        return t.reshape(batch, seq_len, num_heads, head_dim) \\\n",
        "                .transpose(0, 2, 1, 3) \\\n",
        "                .reshape(batch * num_heads, seq_len, head_dim)\n",
        "\n",
        "    q_3d = to_3d_layout(q)\n",
        "    k_3d = to_3d_layout(k)\n",
        "    v_3d = to_3d_layout(v)\n",
        "\n",
        "    # 4. Scaled Dot Product Attention\n",
        "    # Note: Multiply by reciprocal instead of divide (matches IR %10 stablehlo.multiply)\n",
        "    scale = 1.0 / jnp.sqrt(head_dim).astype(q.dtype)\n",
        "    q_scaled = q_3d * scale\n",
        "\n",
        "    # Dot product: (B*H, S, D) @ (B*H, D, S) -> (B*H, S, S)\n",
        "    # Matches IR: stablehlo.dot_general on rank-3 tensors\n",
        "    scores = jnp.matmul(q_scaled, k_3d.swapaxes(-2, -1))\n",
        "\n",
        "    # Softmax\n",
        "    attn_weights = jax.nn.softmax(scores, axis=-1)\n",
        "\n",
        "    # Weighted Sum: (B*H, S, S) @ (B*H, S, D) -> (B*H, S, D)\n",
        "    attn_output_3d = jnp.matmul(attn_weights, v_3d)\n",
        "\n",
        "    # 5. Reshape Back\n",
        "    # (B*H, S, D) -> (B, H, S, D) -> (B, S, H, D) -> (B, S, E)\n",
        "    attn_output = attn_output_3d.reshape(batch, num_heads, seq_len, head_dim) \\\n",
        "                                .transpose(0, 2, 1, 3) \\\n",
        "                                .reshape(batch, seq_len, embed_dim)\n",
        "\n",
        "    # 6. Output Projection + Bias\n",
        "    output = jnp.dot(attn_output, w_o) + b_o\n",
        "\n",
        "    # 7. Return Tuple (Output, Weights)\n",
        "    # PyTorch IR returns both. Weights must be reshaped to match PyTorch return signature.\n",
        "    # PyTorch returns weights as (Batch, Seq, Seq) or (Batch*Heads, Seq, Seq) depending on config.\n",
        "    # The IR provided returned tensor<128x64x64xf32>, which implies averaging over heads\n",
        "    # OR the IR trace was from a specific configuration that reduced it.\n",
        "    # However, to match the raw computation graph, we return the 3D weights or reshape them.\n",
        "    # Here we return the tuple as seen in the PyTorch IR signature.\n",
        "\n",
        "    # Based on IR %50 = multiply(reduce(attn_weights)), PyTorch often returns averaged weights\n",
        "    # if need_weights=True is set but average_attn_weights=True (default).\n",
        "    avg_weights = attn_weights.reshape(batch, num_heads, seq_len, seq_len).sum(axis=1) / num_heads\n",
        "\n",
        "    return output, avg_weights"
      ],
      "metadata": {
        "id": "TGXY93DezN81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_layer(x, w):\n",
        "  \"\"\"\n",
        "    N = batch size\n",
        "    C = input channels\n",
        "    H = input height\n",
        "    W = input width\n",
        "\n",
        "    O = output channels (number of filters)\n",
        "    I = input channels (like RGB)\n",
        "    H = kernel height\n",
        "    W = kernel width\n",
        "\n",
        "    x = input tensor of shape (N, I, H, W) (input height and width)\n",
        "    w = weight tensor of shape (O, I, H, W) (kernel height and width)\n",
        "  \"\"\"\n",
        "  return jax.lax.conv_general_dilated(\n",
        "      lhs=x, rhs=w,\n",
        "      window_strides=(1, 1),\n",
        "      padding='SAME',\n",
        "      dimension_numbers=('NCHW', 'OIHW', 'NCHW')\n",
        "  )\n"
      ],
      "metadata": {
        "id": "GH_Z8jCHtKzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz\n",
        "\n",
        "from pathlib import Path\n",
        "import re\n",
        "from graphviz import Digraph\n",
        "\n",
        "def visualize_mlir(mlir_path, func_name, graph_path):\n",
        "  with open(mlir_path) as f:\n",
        "      text = f.read()\n",
        "\n",
        "  # matches like: %2 = stablehlo.dot_general %0, %1, ...\n",
        "  pattern = re.compile(r\"(%\\w+)\\s*=\\s*([\\w\\.]+)\\s+(.*)\")\n",
        "  ops = []\n",
        "  for line in text.splitlines():\n",
        "      line = line.strip()\n",
        "      m = pattern.match(line)\n",
        "      if not m:\n",
        "          continue\n",
        "      out, op, rest = m.groups()\n",
        "      # Extract inputs like %0, %1\n",
        "      inputs = re.findall(r\"%\\w+\", rest)\n",
        "      ops.append((out, op, inputs))\n",
        "\n",
        "  g = Digraph(\"StableHLO\", format=\"png\")\n",
        "  g.attr(rankdir=\"LR\")\n",
        "\n",
        "  for out, op, inputs in ops:\n",
        "      g.node(out, f\"{out}\\n{op}\")\n",
        "\n",
        "      for inp in inputs:\n",
        "          g.edge(inp, out)\n",
        "\n",
        "  for arg in re.findall(r\"(%arg\\d+):\", text):\n",
        "      g.node(arg, f\"{arg}\\ninput\", shape=\"box\")\n",
        "\n",
        "  ret = re.search(r\"return\\s+(.*)\\s*:\", text)\n",
        "  if ret:\n",
        "      outs = re.findall(r\"%\\w+\", ret.group(1))\n",
        "      for o in outs:\n",
        "          g.edge(o, \"return\")\n",
        "      g.node(\"return\", \"return\", shape=\"box\")\n",
        "\n",
        "  g.render(graph_path, view=True, cleanup=True)\n"
      ],
      "metadata": {
        "id": "ewnn2vlh87wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time, math\n",
        "import jax, os\n",
        "import jax.numpy as jnp\n",
        "import shutil, glob, gzip\n",
        "import numpy as np\n",
        "import json\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "jax.clear_caches()\n",
        "\n",
        "# ---------- helpers ----------\n",
        "\n",
        "def _human_bytes(num: int) -> str:\n",
        "    if num is None:\n",
        "        return \"N/A\"\n",
        "    num = float(num)\n",
        "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n",
        "        if num < 1024.0 or unit == \"TB\":\n",
        "            return f\"{num:.2f} {unit}\"\n",
        "        num /= 1024.0\n",
        "\n",
        "def _is_array_like(x):\n",
        "    return (\n",
        "        isinstance(x, (jnp.ndarray, np.ndarray))\n",
        "        or (hasattr(x, \"shape\") and hasattr(x, \"dtype\") and hasattr(x, \"size\"))\n",
        "    )\n",
        "\n",
        "def _collect_arrays(obj, out_list):\n",
        "    if _is_array_like(obj):\n",
        "        out_list.append(obj)\n",
        "    elif isinstance(obj, (list, tuple)):\n",
        "        for v in obj:\n",
        "            _collect_arrays(v, out_list)\n",
        "    elif isinstance(obj, dict):\n",
        "        for v in obj.values():\n",
        "            _collect_arrays(v, out_list)\n",
        "\n",
        "def _arrays_and_bytes_from_args(args):\n",
        "    \"\"\"Flatten all arrays from *args and compute total size in bytes.\"\"\"\n",
        "    arrays = []\n",
        "    for a in args:\n",
        "        _collect_arrays(a, arrays)\n",
        "    total_bytes = 0\n",
        "    for arr in arrays:\n",
        "        try:\n",
        "            itemsize = int(np.dtype(arr.dtype).itemsize)\n",
        "            size = int(np.prod(arr.shape))\n",
        "            total_bytes += itemsize * size\n",
        "        except Exception:\n",
        "            pass\n",
        "    return arrays, total_bytes\n",
        "\n",
        "def _tree_bytes(x):\n",
        "    \"\"\"Total bytes of all array-like leaves in a PyTree-like object.\"\"\"\n",
        "    arrays = []\n",
        "    _collect_arrays(x, arrays)\n",
        "    total = 0\n",
        "    for arr in arrays:\n",
        "        try:\n",
        "            itemsize = int(np.dtype(arr.dtype).itemsize)\n",
        "            size = int(np.prod(arr.shape))\n",
        "            total += itemsize * size\n",
        "        except Exception:\n",
        "            pass\n",
        "    return total\n",
        "\n",
        "def _infer_throughput_dims_from_args(args):\n",
        "    \"\"\"\n",
        "    Heuristic: inspect all array-like args and try to infer:\n",
        "      - batch size\n",
        "      - tokens per sample (seq len or H*W)\n",
        "      - tokens per step\n",
        "    \"\"\"\n",
        "    arrays, _ = _arrays_and_bytes_from_args(args)\n",
        "    if not arrays:\n",
        "        return None\n",
        "\n",
        "    cand_3d = [a for a in arrays if len(getattr(a, \"shape\", ())) == 3]\n",
        "    cand_4d = [a for a in arrays if len(getattr(a, \"shape\", ())) == 4]\n",
        "    cand_2d = [a for a in arrays if len(getattr(a, \"shape\", ())) == 2]\n",
        "\n",
        "    mode = None\n",
        "    main = None\n",
        "\n",
        "    def numel(a):\n",
        "        try:\n",
        "            return int(np.prod(a.shape))\n",
        "        except Exception:\n",
        "            return 0\n",
        "\n",
        "    # Prefer 3D (B, S, D) -> sequence, then 4D -> image, then 2D -> (B, features)\n",
        "    if cand_3d:\n",
        "        main = max(cand_3d, key=numel)\n",
        "        B, S = int(main.shape[0]), int(main.shape[1])\n",
        "        tokens_per_sample = S\n",
        "        mode = \"sequence_3d\"\n",
        "    elif cand_4d:\n",
        "        main = max(cand_4d, key=numel)\n",
        "        B = int(main.shape[0])\n",
        "        H, W = int(main.shape[-2]), int(main.shape[-1])\n",
        "        tokens_per_sample = H * W\n",
        "        mode = \"image_4d\"\n",
        "    elif cand_2d:\n",
        "        main = max(cand_2d, key=numel)\n",
        "        B, S = int(main.shape[0]), int(main.shape[1])\n",
        "        tokens_per_sample = S\n",
        "        mode = \"sequence_2d\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    tokens_per_step = B * tokens_per_sample\n",
        "    return {\n",
        "        \"batch_size\": B,\n",
        "        \"tokens_per_sample\": tokens_per_sample,\n",
        "        \"tokens_per_step\": tokens_per_step,\n",
        "        \"mode\": mode,\n",
        "        \"example_shape\": tuple(int(x) for x in main.shape),\n",
        "    }\n",
        "\n",
        "# ---------- TPU trace parsing ----------\n",
        "\n",
        "def extract_tpu_ops(trace_json):\n",
        "    fields = [\"name\",  \"timestamp_ns\", \"duration_ns\",\n",
        "              \"bytes_accessed\", \"raw_bytes_accessed\",\n",
        "              \"device_duration_ps\", \"device_offset_ps\",\n",
        "              \"hlo_category\", \"model_flops\", \"long_name\"\n",
        "              \"shape_with_layout\", \"tf_op\"]\n",
        "\n",
        "    tpu_ops = []\n",
        "\n",
        "    for event in trace_json.get(\"traceEvents\", []):\n",
        "        if event.get(\"ph\") != \"X\":\n",
        "            continue\n",
        "\n",
        "        args = event.get(\"args\", {})\n",
        "        if \"device_duration_ps\" not in args and \"hlo_category\" not in args:\n",
        "            continue\n",
        "\n",
        "        op = {\n",
        "            \"name\": event.get(\"name\", \"\"),\n",
        "            \"timestamp_ns\": event.get(\"ts\"),\n",
        "            \"duration_ns\": event.get(\"dur\"),\n",
        "        }\n",
        "\n",
        "        # Add all optional fields from args\n",
        "        for field in [\"bytes_accessed\", \"raw_bytes_accessed\", \"device_duration_ps\",\n",
        "                      \"device_offset_ps\", \"hlo_category\", \"model_flops\",\n",
        "                      \"shape_with_layout\", \"long_name\", \"tf_op\"]:\n",
        "            op[field] = args.get(field)\n",
        "\n",
        "        tpu_ops.append(op)\n",
        "\n",
        "    lines = []\n",
        "    for i, op in enumerate(tpu_ops, 1):\n",
        "        lines.append(f\"--- TPU Op {i} ---\")\n",
        "        for k, v in op.items():\n",
        "            lines.append(f\"{k:20}: {v}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def compute_trace_memory_metrics(trace_json):\n",
        "    \"\"\"\n",
        "    From the TPU trace, estimate memory *traffic* stats using bytes_accessed.\n",
        "    This is NOT peak HBM, but useful for bandwidth-ish insight.\n",
        "    \"\"\"\n",
        "    total_bytes_accessed = 0\n",
        "    max_bytes_accessed = 0\n",
        "    op_count_with_bytes = 0\n",
        "\n",
        "    for event in trace_json.get(\"traceEvents\", []):\n",
        "        if event.get(\"ph\") != \"X\":\n",
        "            continue\n",
        "        args = event.get(\"args\", {})\n",
        "        if \"device_duration_ps\" not in args and \"hlo_category\" not in args:\n",
        "            continue\n",
        "\n",
        "        b = args.get(\"bytes_accessed\")\n",
        "        if b is None:\n",
        "            continue\n",
        "        if isinstance(b, str):\n",
        "            try:\n",
        "                b = int(b)\n",
        "            except ValueError:\n",
        "                continue\n",
        "        try:\n",
        "            b = int(b)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        op_count_with_bytes += 1\n",
        "        total_bytes_accessed += b\n",
        "        if b > max_bytes_accessed:\n",
        "            max_bytes_accessed = b\n",
        "\n",
        "    return {\n",
        "        \"ops_with_bytes_accessed\": op_count_with_bytes,\n",
        "        \"total_bytes_accessed\": total_bytes_accessed,\n",
        "        \"total_bytes_accessed_human\": _human_bytes(total_bytes_accessed),\n",
        "        \"max_bytes_accessed\": max_bytes_accessed,\n",
        "        \"max_bytes_accessed_human\": _human_bytes(max_bytes_accessed),\n",
        "    }\n",
        "\n",
        "# ---------- basic timing helpers ----------\n",
        "\n",
        "# Return function call time in ms.\n",
        "def profile_func_call(func_name, compiled_func, get_trace, *args):\n",
        "    logs_dir = f\"logs/{func_name}\"\n",
        "\n",
        "    # First call will include compile time.\n",
        "    t_s = time.perf_counter()\n",
        "    jax.block_until_ready(compiled_func(*args))\n",
        "    t_e = time.perf_counter()\n",
        "\n",
        "    # Generate trace.\n",
        "    if get_trace:\n",
        "        jax.profiler.start_trace(logs_dir)\n",
        "        jax.block_until_ready(compiled_func(*args))\n",
        "        jax.profiler.stop_trace()\n",
        "\n",
        "    return (t_e - t_s) * 1000\n",
        "\n",
        "def write_trace_summary(root_profile_dir):\n",
        "    trace_files = glob.glob(os.path.join(root_profile_dir, \"**\", \"*.json.gz\"), recursive=True)\n",
        "    if not trace_files:\n",
        "        return None, None  # no trace\n",
        "\n",
        "    with gzip.open(trace_files[0], \"rt\", encoding=\"utf-8\") as f:\n",
        "        trace_json = json.load(f)\n",
        "\n",
        "    summary_text = extract_tpu_ops(trace_json)\n",
        "    summary_path = os.path.join(root_profile_dir, \"summary.txt\")\n",
        "    with open(summary_path, \"w\") as f:\n",
        "        f.write(summary_text)\n",
        "\n",
        "    trace_mem_metrics = compute_trace_memory_metrics(trace_json)\n",
        "    return summary_path, trace_mem_metrics\n",
        "\n",
        "# ---------- main profiling entrypoint ----------\n",
        "\n",
        "def profile_jax_function(func_name, func, num_runs, *args):\n",
        "    \"\"\"\n",
        "    Profile a JAX function on the current backend.\n",
        "\n",
        "    Memory section includes:\n",
        "      - peak_hbm_bytes_est     (approx; based on args)\n",
        "      - weight_bytes           (assumes args[0] is params PyTree)\n",
        "      - activation_bytes_est   (total_arg_bytes - weight_bytes)\n",
        "      - usage_fraction_est     (if JAX_TOTAL_HBM_BYTES env var is set)\n",
        "\n",
        "    Returns:\n",
        "      report: dict with timing, throughput, memory, and artifact paths.\n",
        "    \"\"\"\n",
        "    print(f\"Profiling {func_name}.\\n\")\n",
        "\n",
        "    dir_to_remove = f\"logs/{func_name}\"\n",
        "    if os.path.exists(dir_to_remove):\n",
        "        shutil.rmtree(dir_to_remove)\n",
        "\n",
        "    compiled_func = jax.jit(func)\n",
        "\n",
        "    # --- compile + first execution (also captures a trace) ---\n",
        "    first_call_time = profile_func_call(func_name, compiled_func, True, *args)\n",
        "    print(f\"Run 0 (compile + execute): {first_call_time:.3f} ms.\")\n",
        "\n",
        "    # --- steady-state timings ---\n",
        "    all_times = []\n",
        "    for i in range(num_runs):\n",
        "        run_time = profile_func_call(func_name, compiled_func, False, *args)\n",
        "        all_times.append(run_time)\n",
        "        print(f\"Run {i+1}: {run_time:.3f} ms\")\n",
        "\n",
        "    avg_time = float(np.mean(all_times))\n",
        "    std_time = float(np.std(all_times))\n",
        "    print(f\"\\nAverage over {num_runs} runs: {avg_time:.3f} ms Â± {std_time:.3f} ms.\")\n",
        "\n",
        "    root_profile_dir = f\"{dir_to_remove}/plugins/profile\"\n",
        "\n",
        "    # --- write textual TPU-op summary + trace memory traffic ---\n",
        "    summary_path, trace_mem_metrics = write_trace_summary(root_profile_dir)\n",
        "\n",
        "    # --- StableHLO IR export + graph ---\n",
        "    # stablehlo_ir = compiled_func.lower(*args).compiler_ir(\"stablehlo\")\n",
        "    mlir_path = f\"{root_profile_dir}/stablehlo.txt\"\n",
        "    graph_path = f\"{root_profile_dir}/stablehlo\"\n",
        "\n",
        "    # os.makedirs(root_profile_dir, exist_ok=True)\n",
        "    # with open(mlir_path, \"w\") as f:\n",
        "    #     f.write(str(stablehlo_ir))\n",
        "\n",
        "    # visualize_mlir(mlir_path, func_name, graph_path)\n",
        "\n",
        "    # --- device memory profile (for true HBM usage/fragmentation) ---\n",
        "    memory_prof_path = os.path.join(root_profile_dir, \"memory.prof\")\n",
        "    try:\n",
        "        jax.profiler.save_device_memory_profile(memory_prof_path)\n",
        "    except Exception as e:\n",
        "        memory_prof_path = f\"PROFILE_FAILED: {type(e).__name__}: {e}\"\n",
        "\n",
        "    # ---------- memory section (with the metrics you asked for) ----------\n",
        "\n",
        "    # 1) Total size of all array-like args (params + inputs) as a lower bound on what\n",
        "    #    needs device memory at some point.\n",
        "    _, total_arg_bytes = _arrays_and_bytes_from_args(args)\n",
        "\n",
        "    # 2) Weight bytes: assume first positional argument is a params PyTree.\n",
        "    weight_bytes = 0\n",
        "    if func_name == \"FFN\" or func_name == \"MHA\":\n",
        "        weight_bytes = _tree_bytes(args[1]) + _tree_bytes(args[2]) + _tree_bytes(args[3]) + _tree_bytes(args[4])\n",
        "    elif func_name != \"GEMM\":\n",
        "        weight_bytes = _tree_bytes(args[0])\n",
        "\n",
        "    # 3) Activation bytes (approx): \"everything else\".\n",
        "    activation_bytes_est = max(total_arg_bytes - weight_bytes, 0)\n",
        "\n",
        "    # 4) Peak HBM (approx): weights + these activations.\n",
        "    peak_hbm_bytes_est = weight_bytes + activation_bytes_est\n",
        "\n",
        "    # 5) Total HBM capacity (optional, from env), and usage fraction.\n",
        "    total_hbm_bytes = None\n",
        "    env_val = os.environ.get(\"JAX_TOTAL_HBM_BYTES\")\n",
        "    if env_val is not None:\n",
        "        try:\n",
        "            total_hbm_bytes = int(env_val)\n",
        "        except ValueError:\n",
        "            total_hbm_bytes = None\n",
        "\n",
        "    usage_fraction_est = (\n",
        "        (peak_hbm_bytes_est / total_hbm_bytes) if (total_hbm_bytes and peak_hbm_bytes_est) else None\n",
        "    )\n",
        "\n",
        "    memory_summary = {\n",
        "        # Core metrics you asked for:\n",
        "        \"peak_hbm_bytes_est\": peak_hbm_bytes_est,\n",
        "        \"peak_hbm_human\": _human_bytes(peak_hbm_bytes_est),\n",
        "        \"weight_bytes\": weight_bytes,\n",
        "        \"weight_human\": _human_bytes(weight_bytes),\n",
        "        \"activation_bytes_est\": activation_bytes_est,\n",
        "        \"activation_human\": _human_bytes(activation_bytes_est),\n",
        "        \"usage_fraction_est\": usage_fraction_est,\n",
        "\n",
        "        # Context:\n",
        "        \"total_hbm_bytes_config\": total_hbm_bytes,\n",
        "        \"total_hbm_human\": _human_bytes(total_hbm_bytes) if total_hbm_bytes else None,\n",
        "        \"arg_total_bytes\": total_arg_bytes,\n",
        "        \"arg_total_human\": _human_bytes(total_arg_bytes),\n",
        "        \"note\": (\n",
        "            \"peak_hbm_bytes_est is a heuristic based on argument sizes. \"\n",
        "            \"True peak HBM and fragmentation should be read from memory.prof \"\n",
        "            \"via pprof or TensorBoard.\"\n",
        "        ),\n",
        "        \"trace_memory\": trace_mem_metrics,\n",
        "    }\n",
        "\n",
        "    # --- throughput metrics (tokens/s, images/s) ---\n",
        "    throughput_dims = _infer_throughput_dims_from_args(args)\n",
        "    throughput_summary = None\n",
        "    if throughput_dims is not None and avg_time > 0:\n",
        "        avg_step_s = avg_time / 1000.0\n",
        "        B = throughput_dims[\"batch_size\"]\n",
        "        tokens_per_step = throughput_dims[\"tokens_per_step\"]\n",
        "        mode = throughput_dims[\"mode\"]\n",
        "\n",
        "        tokens_per_second = tokens_per_step / avg_step_s\n",
        "        images_per_second = None\n",
        "        if mode == \"image_4d\":\n",
        "            images_per_second = B / avg_step_s\n",
        "\n",
        "        throughput_summary = {\n",
        "            \"mode\": mode,\n",
        "            \"batch_size\": B,\n",
        "            \"tokens_per_sample\": throughput_dims[\"tokens_per_sample\"],\n",
        "            \"tokens_per_step\": tokens_per_step,\n",
        "            \"avg_step_ms\": avg_time,\n",
        "            \"tokens_per_second\": tokens_per_second,\n",
        "            \"images_per_second\": images_per_second,\n",
        "            \"example_input_shape\": throughput_dims[\"example_shape\"],\n",
        "        }\n",
        "\n",
        "        print(\"\\nThroughput estimate:\")\n",
        "        print(f\"  Mode: {mode}\")\n",
        "        print(f\"  Batch size: {B}\")\n",
        "        print(f\"  Tokens per step: {tokens_per_step}\")\n",
        "        print(f\"  Tokens/sec (avg): {tokens_per_second:,.2f}\")\n",
        "        if images_per_second is not None:\n",
        "            print(f\"  Images/sec (avg): {images_per_second:,.2f}\")\n",
        "    else:\n",
        "        print(\"\\nThroughput estimate: could not infer from argument shapes.\")\n",
        "\n",
        "    # --- pretty-print some memory stats ---\n",
        "    print(\"\\nMemory estimates:\")\n",
        "    print(f\"  Weight bytes:          {memory_summary['weight_human']}\")\n",
        "    print(f\"  Activation bytes (est):{memory_summary['activation_human']}\")\n",
        "    print(f\"  Peak HBM (est):        {memory_summary['peak_hbm_human']}\")\n",
        "    if memory_summary[\"total_hbm_human\"]:\n",
        "        print(f\"  Total HBM (config):    {memory_summary['total_hbm_human']}\")\n",
        "        print(f\"  Usage fraction (est):  {memory_summary['usage_fraction_est']:.3f}\")\n",
        "    else:\n",
        "        print(\"  Total HBM unknown (set JAX_TOTAL_HBM_BYTES to get usage fraction).\")\n",
        "\n",
        "    if trace_mem_metrics is not None:\n",
        "        print(\"\\nTrace memory traffic (bytes_accessed across TPU ops):\")\n",
        "        print(f\"  Ops with bytes_accessed: {trace_mem_metrics['ops_with_bytes_accessed']}\")\n",
        "        print(f\"  Total bytes_accessed:    {trace_mem_metrics['total_bytes_accessed_human']}\")\n",
        "        print(f\"  Max bytes_accessed/op:   {trace_mem_metrics['max_bytes_accessed_human']}\")\n",
        "\n",
        "    print(\"\\nArtifacts:\")\n",
        "    print(f\"  TPU trace summary: {summary_path}\")\n",
        "    print(f\"  StableHLO IR:      {mlir_path}\")\n",
        "    print(f\"  StableHLO graph:   {graph_path}\")\n",
        "    print(f\"  Device memory profile (HBM/fragmentation via pprof/TensorBoard): {memory_prof_path}\")\n",
        "\n",
        "    report = {\n",
        "        \"func_name\": func_name,\n",
        "        \"timing_ms\": {\n",
        "            \"first_run_ms\": first_call_time,\n",
        "            \"runs_ms\": all_times,\n",
        "            \"mean_ms\": avg_time,\n",
        "            \"std_ms\": std_time,\n",
        "        },\n",
        "        \"throughput\": throughput_summary,\n",
        "        \"memory\": memory_summary,\n",
        "        \"paths\": {\n",
        "            \"trace_summary_txt\": summary_path,\n",
        "            \"stablehlo_txt\": mlir_path,\n",
        "            \"stablehlo_graph_dir\": graph_path,\n",
        "            \"device_memory_profile\": memory_prof_path,\n",
        "            \"tensorboard_logdir\": f\"logs/{func_name}\",\n",
        "        },\n",
        "    }\n",
        "\n",
        "    print(\"\\nJSON report:\")\n",
        "    print(json.dumps(report, indent=2, default=str))\n",
        "\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "xm1c2u4ov_WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = random.split(master_key, 2)\n",
        "\n",
        "A = random.normal(keys[0], (128, 256))\n",
        "B = random.normal(keys[1], (256, 1024))\n",
        "\n",
        "profile_jax_function(\"GEMM\", gemm, num_runs, A, B)\n",
        "\n",
        "# Trace had two steps: copy-done and fusion.\n",
        "# Copy seemed to only moves tensor B (GPT says it's rearranging the tensor for more efficient computation).\n",
        "  # %copy-done = f32[256,1024]{1,0:T(8,128)S(1)} copy-done((f32[256,1024]{1,0:T(8,128)S(1)}, f32[256,1024]{1,0:T(8,128)}, u32[]{:S(2)}) %copy-start)\n",
        "# The fusion read all of A and B and wrote the output.\n",
        "  # %fusion = f32[128,1024]{1,0:T(8,128)} fusion(f32[128,256]{1,0:T(8,128)} %Arg_0.1, f32[256,1024]{1,0:T(8,128)S(1)} %copy-done), kind=kOutput, calls=%fused_computation"
      ],
      "metadata": {
        "id": "cddHpZdPwmOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = random.split(master_key, 5)\n",
        "\n",
        "x = random.normal(keys[0], (128, 64, 256))\n",
        "w1 = random.normal(keys[1], (256, 1024))\n",
        "b1 = random.normal(keys[2], (1024,))\n",
        "w2 = random.normal(keys[3], (1024, 256))\n",
        "b2 = random.normal(keys[4], (256,))\n",
        "\n",
        "profile_jax_function(\"FFN\", ffn, num_runs, x, w1, b1, w2, b2)\n",
        "\n",
        "# First matrix multiplication is broken up into fusion and copy.1\n",
        "  # %copy-done = f32[256,1024]{1,0:T(8,128)S(1)} copy-done((f32[256,1024]{1,0:T(8,128)S(1)}, f32[256,1024]{1,0:T(8,128)}, u32[]{:S(2)}) %copy-start)\n",
        "  # %fusion = f32[128,64,1024]{2,0,1:T(8,128)S(1)} fusion(f32[128,64,256]{2,1,0:T(8,128)} %Arg_0.1, f32[256,1024,1]{1,0,2:T(8,128)S(1)} %bitcast.8), kind=kOutput, calls=%fused_computation\n",
        "  # %copy.1 = f32[128,64,1024]{2,1,0:T(8,128)} copy(f32[128,64,1024]{2,0,1:T(8,128)S(1)} %fusion)\n",
        "# We add the first bias.\n",
        "  # %broadcast_add_fusion = f32[128,64,1024]{2,1,0:T(8,128)} fusion(f32[128,64,1024]{2,1,0:T(8,128)} %Arg_0.1, f32[1024]{0:T(1024)} %Arg_1.2), kind=kLoop, calls=%fused_computation\n",
        "# Some stuff happens on the CPU in between.\n",
        "  # %broadcast_maximum_fusion = f32[128,64,1024]{2,1,0:T(8,128)} fusion(f32[128,64,1024]{2,1,0:T(8,128)} %Arg_0.1), kind=kLoop, calls=%fused_computation\n",
        "# Then the second matmul.\n",
        "  # %fusion.3 = f32[128,64,256]{2,1,0:T(8,128)} fusion(f32[128,64,1024]{2,1,0:T(8,128)S(1)} %copy-done, f32[1024,256,1]{1,0,2:T(8,128)} %bitcast.9), kind=kOutput, calls=%fused_computation.2\n",
        "# Then the second bias is added.\n",
        "  # %broadcast_add_fusion = f32[128,64,256]{2,1,0:T(8,128)} fusion(f32[128,64,256]{2,1,0:T(8,128)} %Arg_0.1, f32[256]{0:T(256)} %Arg_1.2), kind=kLoop, calls=%fused_computation\n",
        "\n",
        "# Maybe missing some details."
      ],
      "metadata": {
        "id": "1dVPoS5swqk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = random.split(master_key, 5)\n",
        "\n",
        "batch, seq, embed, heads = 128, 64, 256, 16\n",
        "\n",
        "x = random.normal(keys[0], (batch, seq, embed))\n",
        "w_q = random.normal(keys[1], (embed, embed))\n",
        "w_k = random.normal(keys[2], (embed, embed))\n",
        "w_v = random.normal(keys[3], (embed, embed))\n",
        "w_o = random.normal(keys[4], (embed, embed))\n",
        "\n",
        "profile_jax_function(\"MHA\", mha, num_runs, x, w_q, w_k, w_v, w_o)"
      ],
      "metadata": {
        "id": "gZq1kWmYwqZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using built-in JAX convolution (probably why it's faster)\n",
        "\n",
        "keys = random.split(master_key, 2)\n",
        "\n",
        "batch, in_channels, input_height, input_width = 128, 3, 12, 12\n",
        "out_channels, kernel_height, kernel_width = 3, 3, 3\n",
        "\n",
        "# padding and stride are 1 (implicit in the function)\n",
        "\n",
        "x = random.normal(keys[0], (batch, in_channels, input_height, input_width))   # NCHW\n",
        "w = random.normal(keys[1], (out_channels, in_channels, kernel_height, kernel_width))       # OIHW\n",
        "\n",
        "profile_jax_function(\"Conv\", conv_layer, num_runs, x, w)"
      ],
      "metadata": {
        "id": "48oP3lAawqNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_image_model_jax_fn(model_name, batch_size=8, height=224, width=224):\n",
        "    \"\"\"\n",
        "    Build an image classification model as a JAX fn.\n",
        "\n",
        "    If a Flax version exists (ViT), use Flax.\n",
        "    Otherwise (e.g., ResNet-50), fall back to torchax on PyTorch.\n",
        "\n",
        "    Returns:\n",
        "      jax_forward(params_or_weights, pixel_values),\n",
        "      params_or_weights,\n",
        "      dummy_pixel_values\n",
        "    \"\"\"\n",
        "    # Dummy image batch: NCHW, as HF vision models typically expect\n",
        "    dummy_pixel_values = jnp.ones(\n",
        "        (batch_size, 3, height, width), dtype=jnp.float32\n",
        "    )\n",
        "\n",
        "    # ---- Flax ViT path ----\n",
        "    if model_name == \"google/vit-base-patch16-224\":\n",
        "        model = FlaxViTForImageClassification.from_pretrained(model_name)\n",
        "\n",
        "        params = model.params\n",
        "\n",
        "        def jax_forward(params, pixel_values):\n",
        "            outputs = model(\n",
        "                pixel_values=pixel_values,\n",
        "                params=params,\n",
        "                train=False,\n",
        "            )\n",
        "            # logits is a single JAX array\n",
        "            return outputs.logits\n",
        "\n",
        "        print(\"Using Flax!\")\n",
        "        return jax_forward, params, dummy_pixel_values\n",
        "\n",
        "    # ---- Fallback: PyTorch ResNet-50 via torchax ----\n",
        "    else:\n",
        "        pt_model = AutoModelForImageClassification.from_pretrained(model_name)\n",
        "        weights, raw_func = torchax.extract_jax(pt_model)\n",
        "\n",
        "        def jax_forward(weights, pixel_values):\n",
        "            # return_dict=False -> tuple of tensors; first element is logits\n",
        "            logits, *_ = raw_func(\n",
        "                weights,\n",
        "                (),\n",
        "                {\"pixel_values\": pixel_values, \"return_dict\": False},\n",
        "            )\n",
        "            return logits\n",
        "\n",
        "        print(\"Using PyTorch and torchax!\")\n",
        "        return jax_forward, weights, dummy_pixel_values"
      ],
      "metadata": {
        "id": "SUr-I0X2Y2Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Vision: ResNet-50\n",
        "print(\"\\n================ Vision: ResNet-50 ================\")\n",
        "resnet_fn, resnet_params, resnet_pixels = make_image_model_jax_fn(\n",
        "    \"microsoft/resnet-50\",\n",
        "    batch_size=8,\n",
        "    height=224,\n",
        "    width=224,\n",
        ")\n",
        "profile_jax_function(\n",
        "    \"ResNet-50\",\n",
        "    resnet_fn,\n",
        "    num_runs,\n",
        "    resnet_params,\n",
        "    resnet_pixels,\n",
        ")"
      ],
      "metadata": {
        "id": "RNk-zy3BKE9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2) Vision: ViT-B/16\n",
        "print(\"\\n================ Vision: ViT-B/16 ================\")\n",
        "vit_fn, vit_params, vit_pixels = make_image_model_jax_fn(\n",
        "    \"google/vit-base-patch16-224\",\n",
        "    batch_size=8,\n",
        "    height=224,\n",
        "    width=224,\n",
        ")\n",
        "profile_jax_function(\n",
        "    \"ViT-B-16\",\n",
        "    vit_fn,\n",
        "    num_runs,\n",
        "    vit_params,\n",
        "    vit_pixels,\n",
        ")"
      ],
      "metadata": {
        "id": "STI9wnmvKMp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_bert_flax_jax_fn(model_name=\"bert-base-uncased\", batch_size=4, seq_len=128):\n",
        "    \"\"\"\n",
        "    Build a BERT-base sequence classifier using Hugging Face's Flax model,\n",
        "    and return a function compatible with profile_jax_function:\n",
        "\n",
        "      jax_forward(params, input_ids, attention_mask)\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = FlaxBertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "    encoded = tokenizer(\n",
        "        [\"this is a dummy sentence\"] * batch_size,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=seq_len,\n",
        "        return_tensors=\"jax\",\n",
        "    )\n",
        "    input_ids = encoded[\"input_ids\"]          # jnp.int32\n",
        "    attention_mask = encoded[\"attention_mask\"]  # jnp.int32\n",
        "\n",
        "    params = model.params  # Flax parameters pytree\n",
        "\n",
        "    def jax_forward(params, input_ids, attention_mask):\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            params=params,\n",
        "            train=False,\n",
        "        )\n",
        "        return outputs.logits  # [batch, num_labels]\n",
        "\n",
        "    return jax_forward, params, input_ids, attention_mask"
      ],
      "metadata": {
        "id": "oGD96d3LP3Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) NLP: BERT-base (sequence classification)\n",
        "print(\"\\n================ NLP: BERT-base (Flax) ================\")\n",
        "bert_fn, bert_params, bert_ids, bert_mask = make_bert_flax_jax_fn(\n",
        "    model_name=\"bert-base-uncased\",\n",
        "    batch_size=4,\n",
        "    seq_len=128,\n",
        ")\n",
        "\n",
        "profile_jax_function(\n",
        "    \"BERT-base-Flax\",\n",
        "    bert_fn,\n",
        "    num_runs,\n",
        "    bert_params,\n",
        "    bert_ids,\n",
        "    bert_mask,\n",
        ")"
      ],
      "metadata": {
        "id": "mjEw3WWGKPZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_gpt_like_jax_fn(\n",
        "    model_name,\n",
        "    batch_size=2,\n",
        "    seq_len=64,\n",
        "    vocab_example_text=\"The quick brown fox jumps over the lazy dog.\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a GPT-like CausalLM:\n",
        "\n",
        "    - If Flax version exists (GPT-2, GPT-Neo), use Flax*ForCausalLM.\n",
        "    - Else (e.g., LLaMA-3.1-8B), fall back to torchax on PyTorch.\n",
        "\n",
        "    Returns:\n",
        "      jax_forward(params_or_weights, input_ids, attention_mask),\n",
        "      params_or_weights,\n",
        "      input_ids,\n",
        "      attention_mask\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "    encoded = tokenizer(\n",
        "        [vocab_example_text] * batch_size,\n",
        "        padding=\"max_length\",\n",
        "        max_length=seq_len,\n",
        "        truncation=True,\n",
        "        return_tensors=\"jax\",\n",
        "    )\n",
        "    input_ids = encoded[\"input_ids\"]\n",
        "    attention_mask = encoded[\"attention_mask\"]\n",
        "\n",
        "    # ---- Flax GPT-2 ----\n",
        "    if model_name == \"gpt2\":\n",
        "        model = FlaxGPT2LMHeadModel.from_pretrained(model_name)\n",
        "        params = model.params\n",
        "\n",
        "        def jax_forward(params, input_ids, attention_mask):\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                params=params,\n",
        "                train=False,\n",
        "            )\n",
        "            return outputs.logits  # [batch, seq, vocab]\n",
        "\n",
        "        print(\"Using Flax!\")\n",
        "        return jax_forward, params, input_ids, attention_mask\n",
        "\n",
        "    # ---- Flax GPT-Neo ----\n",
        "    if model_name == \"EleutherAI/gpt-neo-1.3B\":\n",
        "        model = FlaxGPTNeoForCausalLM.from_pretrained(model_name)\n",
        "        params = model.params\n",
        "\n",
        "        def jax_forward(params, input_ids, attention_mask):\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                params=params,\n",
        "                train=False,\n",
        "            )\n",
        "            return outputs.logits\n",
        "\n",
        "        print(\"Using Flax!\")\n",
        "        return jax_forward, params, input_ids, attention_mask\n",
        "\n",
        "    # ---- Fallback: PyTorch CausalLM via torchax (e.g., LLaMA-3.1-8B) ----\n",
        "    pt_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=None, token=hf_token)\n",
        "    weights, raw_func = torchax.extract_jax(pt_model)\n",
        "\n",
        "    def jax_forward(weights, input_ids, attention_mask):\n",
        "        # Disable cache and return_dict to avoid custom output types.\n",
        "        logits, *_ = raw_func(\n",
        "            weights,\n",
        "            (),\n",
        "            {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"use_cache\": False,\n",
        "                \"return_dict\": False,\n",
        "            },\n",
        "        )\n",
        "        return logits\n",
        "\n",
        "    print(\"Using PyTorch and torchax!\")\n",
        "    return jax_forward, weights, input_ids, attention_mask"
      ],
      "metadata": {
        "id": "79G22oPqP7N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4) NLP: GPT-2 small\n",
        "print(\"\\n================ NLP: GPT-2 small (Flax) ================\")\n",
        "gpt2_fn, gpt2_params, gpt2_ids, gpt2_mask = make_gpt_like_jax_fn(\n",
        "    model_name=\"gpt2\",\n",
        "    batch_size=2,\n",
        "    seq_len=64,\n",
        "    vocab_example_text=\"The quick brown fox jumps over the lazy dog.\",\n",
        ")\n",
        "profile_jax_function(\n",
        "    \"GPT-2-small-Flax\",\n",
        "    gpt2_fn,\n",
        "    num_runs,\n",
        "    gpt2_params,\n",
        "    gpt2_ids,\n",
        "    gpt2_mask,\n",
        ")\n"
      ],
      "metadata": {
        "id": "4lT-daw2KV28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) NLP: GPT-Neo 1.3B\n",
        "print(\"\\n================ NLP: GPT-Neo-1.3B (Flax) ================\")\n",
        "gptneo_fn, gptneo_params, gptneo_ids, gptneo_mask = make_gpt_like_jax_fn(\n",
        "    model_name=\"EleutherAI/gpt-neo-1.3B\",\n",
        "    batch_size=1,       # keep small, it's big\n",
        "    seq_len=64,\n",
        "    vocab_example_text=\"Dynamic pruning and batching for large language models.\",\n",
        ")\n",
        "profile_jax_function(\n",
        "    \"GPT-Neo-1.3B-Flax\",\n",
        "    gptneo_fn,\n",
        "    num_runs,\n",
        "    gptneo_params,\n",
        "    gptneo_ids,\n",
        "    gptneo_mask,\n",
        ")"
      ],
      "metadata": {
        "id": "9QTcHuwbKX0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6) NLP: LLaMA-3.1 8B\n",
        "print(\"\\n================ NLP: LLaMA-3.1-8B (torchax) ================\")\n",
        "llama_fn, llama_weights, llama_ids, llama_mask = make_gpt_like_jax_fn(\n",
        "    model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    batch_size=1,\n",
        "    seq_len=64,\n",
        "    vocab_example_text=\"Large language models can be pruned dynamically per prompt.\",\n",
        ")\n",
        "profile_jax_function(\n",
        "    \"LLaMA-3.1-8B\",\n",
        "    llama_fn,\n",
        "    num_runs,\n",
        "    llama_weights,\n",
        "    llama_ids,\n",
        "    llama_mask,\n",
        ")\n"
      ],
      "metadata": {
        "id": "mLWDZLFdKZLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_unet_jax_fn(\n",
        "    model_name=\"runwayml/stable-diffusion-v1-5\",\n",
        "    batch_size=2,\n",
        "    height=64,\n",
        "    width=64,\n",
        "    channels=4,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a Stable Diffusion UNet block.\n",
        "\n",
        "    If FlaxUNet2DConditionModel is available, use that.\n",
        "    Otherwise, fall back to PyTorch UNet2DConditionModel + torchax.\n",
        "    \"\"\"\n",
        "    if not HAS_DIFFUSERS:\n",
        "        raise ImportError(\"diffusers is not installed, cannot create UNet model.\")\n",
        "\n",
        "    # Dummy latent + timestep + encoder hidden states\n",
        "    sample = jnp.ones(\n",
        "        (batch_size, channels, height, width), dtype=jnp.float32\n",
        "    )\n",
        "    timesteps = jnp.array([1.0] * batch_size, dtype=jnp.float32)\n",
        "    encoder_hidden_states = jnp.ones(\n",
        "        (batch_size, 77, 768), dtype=jnp.float32\n",
        "    )  # typical CLIP text shape\n",
        "\n",
        "    # ---- Flax UNet if available ----\n",
        "    try:\n",
        "        flax_unet = FlaxUNet2DConditionModel.from_pretrained(\n",
        "            model_name,\n",
        "            subfolder=\"unet\",\n",
        "            from_pt=True  # load PT weights into Flax\n",
        "        )\n",
        "        params = flax_unet.params\n",
        "\n",
        "        def jax_forward(params, sample, timesteps, encoder_hidden_states):\n",
        "            outputs = flax_unet(\n",
        "                sample=sample,\n",
        "                timestep=timesteps,\n",
        "                encoder_hidden_states=encoder_hidden_states,\n",
        "                params=params,\n",
        "                train=False,\n",
        "            )\n",
        "            # Flax UNet returns dict-like; we just use sample\n",
        "            return outputs.sample\n",
        "\n",
        "        print(\"Using Flax!\")\n",
        "        return jax_forward, params, sample, timesteps, encoder_hidden_states\n",
        "\n",
        "    except Exception:\n",
        "        # ---- Fallback: PyTorch UNet via torchax ----\n",
        "        unet = UNet2DConditionModel.from_pretrained(\n",
        "            model_name,\n",
        "            subfolder=\"unet\"\n",
        "        )\n",
        "        weights, raw_func = torchax.extract_jax(unet)\n",
        "\n",
        "        def jax_forward(weights, sample, timesteps, encoder_hidden_states):\n",
        "            (out_sample,) = raw_func(\n",
        "                weights,\n",
        "                (),\n",
        "                {\n",
        "                    \"sample\": sample,\n",
        "                    \"timestep\": timesteps,\n",
        "                    \"encoder_hidden_states\": encoder_hidden_states,\n",
        "                    \"return_dict\": False,\n",
        "                },\n",
        "            )\n",
        "            return out_sample\n",
        "\n",
        "        print(\"Using PyTorch and torchax!\")\n",
        "        return jax_forward, weights, sample, timesteps, encoder_hidden_states"
      ],
      "metadata": {
        "id": "jEsSqLDxP_KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diffusion: UNet (Stable Diffusion)\n",
        "print(\"\\n================ Diffusion: UNet (Stable Diffusion) ================\")\n",
        "unet_fn, unet_params, sample, timesteps, enc_hid = make_unet_jax_fn(\n",
        "    model_name=\"runwayml/stable-diffusion-v1-5\",\n",
        "    batch_size=1,\n",
        "    height=64,\n",
        "    width=64,\n",
        "    channels=4,\n",
        ")\n",
        "profile_jax_function(\n",
        "    \"UNet-StableDiffusion\",\n",
        "    unet_fn,\n",
        "    num_runs,\n",
        "    unet_params,\n",
        "    sample,\n",
        "    timesteps,\n",
        "    enc_hid,\n",
        ")"
      ],
      "metadata": {
        "id": "Gs3tSod5KaMX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}