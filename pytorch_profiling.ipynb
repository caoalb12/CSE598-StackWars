{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# pip uninstall -y torch torchvision torchaudio torch_xla\n",
        "pip install -q \"torch==2.8.*\" \"torchvision==0.23.*\" \"torchaudio==2.8.*\"\n",
        "pip install -q \"torch_xla[tpu]==2.8.*\"\n"
      ],
      "metadata": {
        "id": "F576zCxQ0trJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "pip install diffusers"
      ],
      "metadata": {
        "id": "V7EluGRj0iDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, json, statistics, pathlib, re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "from torch_xla.debug import profiler as xp\n",
        "from torch_xla.debug import metrics as xmetrics\n",
        "from torch.profiler import profile, ProfilerActivity\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoModelForImageClassification,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "from diffusers import UNet2DConditionModel\n"
      ],
      "metadata": {
        "id": "_rWjjJZD2gtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch_xla.device()\n",
        "print(device)\n",
        "\n",
        "# device2 = xm.xla_device()\n",
        "# print(device2)\n",
        "\n",
        "print(\"Dynamo backends:\", torch._dynamo.list_backends())"
      ],
      "metadata": {
        "id": "kupQ0Tmt4vx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _human_bytes(n):\n",
        "    for u in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\",\"PB\"]:\n",
        "        if n < 1024:\n",
        "            return f\"{n:.2f} {u}\"\n",
        "        n /= 1024\n",
        "    return f\"{n:.2f} EB\"\n",
        "\n",
        "def _summarize_times(series_ms):\n",
        "    if not series_ms:\n",
        "        return {}\n",
        "    s = sorted(series_ms)\n",
        "    idx95 = max(0, min(len(s)-1, int(0.95*len(s))-1))\n",
        "    return {\n",
        "        \"count\": len(series_ms),\n",
        "        \"mean_ms\": statistics.mean(series_ms),\n",
        "        \"p50_ms\": statistics.median(series_ms),\n",
        "        \"p95_ms\": s[idx95],\n",
        "        \"min_ms\": min(series_ms),\n",
        "        \"max_ms\": max(series_ms),\n",
        "    }"
      ],
      "metadata": {
        "id": "rhGQ-9wu4-_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _export_stablehlo(module_cpu, input_args_cpu, input_kwargs_cpu, out_path):\n",
        "    \"\"\"\n",
        "    Export StableHLO for a (CPU) module using torch.export → save_as_stablehlo.\n",
        "    We use CPU & float32 for widest compatibility.\n",
        "    \"\"\"\n",
        "    # Prefer torch.export; fall back to torch._export for older PyTorch.\n",
        "    try:\n",
        "        from torch.export import export as _export\n",
        "    except Exception:\n",
        "        from torch._export import export as _export\n",
        "\n",
        "    from torch_xla.stablehlo import save_as_stablehlo\n",
        "\n",
        "    # Ensure inputs are tensors on CPU (convert bf16/fp16 to fp32 for export robustness)\n",
        "    def _to_cpu_fp32(x):\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            return x.detach().to(\"cpu\", dtype=torch.float32)\n",
        "        return x\n",
        "\n",
        "    args_cpu = tuple(_to_cpu_fp32(a) for a in input_args_cpu)\n",
        "    kwargs_cpu = {k: _to_cpu_fp32(v) for k, v in input_kwargs_cpu.items()}\n",
        "\n",
        "    module_cpu = module_cpu.to(\"cpu\", dtype=torch.float32).eval()\n",
        "    with torch.no_grad():\n",
        "        ep = _export(module_cpu, args_cpu, kwargs_cpu)\n",
        "    save_as_stablehlo(ep, out_path)\n",
        "    return out_path"
      ],
      "metadata": {
        "id": "En2To0KK5Cas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz\n",
        "\n",
        "from pathlib import Path\n",
        "import re\n",
        "from graphviz import Digraph\n",
        "\n",
        "def visualize_mlir(mlir_path, func_name, graph_path):\n",
        "  pass\n",
        "  # with open(mlir_path) as f:\n",
        "  #     text = f.read()\n",
        "\n",
        "  # # matches like: %2 = stablehlo.dot_general %0, %1, ...\n",
        "  # pattern = re.compile(r\"(%\\w+)\\s*=\\s*([\\w\\.]+)\\s+(.*)\")\n",
        "  # ops = []\n",
        "  # for line in text.splitlines():\n",
        "  #     line = line.strip()\n",
        "  #     m = pattern.match(line)\n",
        "  #     if not m:\n",
        "  #         continue\n",
        "  #     out, op, rest = m.groups()\n",
        "  #     # Extract inputs like %0, %1\n",
        "  #     inputs = re.findall(r\"%\\w+\", rest)\n",
        "  #     ops.append((out, op, inputs))\n",
        "\n",
        "  # g = Digraph(\"StableHLO\", format=\"png\")\n",
        "  # g.attr(rankdir=\"LR\")\n",
        "\n",
        "  # for out, op, inputs in ops:\n",
        "  #     g.node(out, f\"{out}\\n{op}\")\n",
        "\n",
        "  #     for inp in inputs:\n",
        "  #         g.edge(inp, out)\n",
        "\n",
        "  # for arg in re.findall(r\"(%arg\\d+):\", text):\n",
        "  #     g.node(arg, f\"{arg}\\ninput\", shape=\"box\")\n",
        "\n",
        "  # ret = re.search(r\"return\\s+(.*)\\s*:\", text)\n",
        "  # if ret:\n",
        "  #     outs = re.findall(r\"%\\w+\", ret.group(1))\n",
        "  #     for o in outs:\n",
        "  #         g.edge(o, \"return\")\n",
        "  #     g.node(\"return\", \"return\", shape=\"box\")\n",
        "\n",
        "  # g.render(graph_path, view=True, cleanup=True)\n"
      ],
      "metadata": {
        "id": "TFu9VBn7Ad1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GEMM\n",
        "\n",
        "def gemm_module_fn():\n",
        "    # No parameters needed — GEMM is just an operation, not a module\n",
        "    def _build():\n",
        "        # Wrap matmul as a callable \"module\" for consistency\n",
        "        class GEMMOp(nn.Module):\n",
        "            def forward(self, a, b):\n",
        "                return a.matmul(b)\n",
        "        return GEMMOp()\n",
        "    return _build\n",
        "\n",
        "\n",
        "def gemm_input_fn(batch_m=128, hidden_dim=256, out_dim=1024):\n",
        "    def _make(device, dt):\n",
        "        a = torch.randn(batch_m, hidden_dim, device=device, dtype=dt)\n",
        "        b = torch.randn(hidden_dim, out_dim, device=device, dtype=dt)\n",
        "        return (a, b), {}   # (args, kwargs)\n",
        "    return _make\n"
      ],
      "metadata": {
        "id": "VeFiYmOIzl6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-head attention\n",
        "\n",
        "def mha_module_fn(embed_dim=256, num_heads=16, batch_first=True, dropout=0.0):\n",
        "    def _build():\n",
        "        return nn.MultiheadAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=batch_first,\n",
        "            dropout=dropout,\n",
        "            bias=True,\n",
        "        )\n",
        "    return _build\n",
        "\n",
        "def mha_input_fn(batch=128, seq_len=64, embed_dim=256):\n",
        "    def _make(device, dt):\n",
        "        x = torch.randn(batch, seq_len, embed_dim, device=device, dtype=dt)\n",
        "        return (x, x, x), {}   # (q,k,v), no kwargs\n",
        "    return _make\n"
      ],
      "metadata": {
        "id": "DKNGvZAizwXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN\n",
        "def conv_module_fn(in_ch=3, out_ch=3, kernel_size=3, stride=1, padding=1):\n",
        "    def _build():\n",
        "        return nn.Conv2d(\n",
        "            in_channels=in_ch,\n",
        "            out_channels=out_ch,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            bias=True,\n",
        "        )\n",
        "    return _build\n",
        "\n",
        "\n",
        "def conv_input_fn(batch=128, in_ch=3, height=12, width=12):\n",
        "    def _make(device, dt):\n",
        "        x = torch.randn(batch, in_ch, height, width, device=device, dtype=dt)\n",
        "        return (x,), {}   # single tensor arg, no kwargs\n",
        "    return _make"
      ],
      "metadata": {
        "id": "pEO0fl4Rz8bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ffn_module_fn(embed_dim=256, hidden_dim=1024):\n",
        "    def _build():\n",
        "        class FFN(nn.Module):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.w1 = nn.Linear(embed_dim, hidden_dim)\n",
        "                self.w2 = nn.Linear(hidden_dim, embed_dim)\n",
        "\n",
        "            def forward(self, x):\n",
        "                # x: (batch, seq_len, embed_dim)\n",
        "                x2 = F.relu(self.w1(x))\n",
        "                x2 = self.w2(x2)\n",
        "                return x2\n",
        "\n",
        "        return FFN()\n",
        "    return _build\n",
        "\n",
        "\n",
        "def ffn_input_fn(batch=128, seq_len=64, embed_dim=256):\n",
        "    def _make(device, dt):\n",
        "        x = torch.randn(batch, seq_len, embed_dim, device=device, dtype=dt)\n",
        "        return (x,), {}   # (args, kwargs)\n",
        "    return _make\n"
      ],
      "metadata": {
        "id": "DUUhXPhK0Bd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def profile_module_on_tpu(\n",
        "    module_fn,\n",
        "    input_fn,\n",
        "    *,\n",
        "    iters=20,\n",
        "    warmup=5,\n",
        "    dtype=\"bf16\",          # 'bf16' (recommended on TPU) or 'fp32'\n",
        "    do_backward=False,     # include backward() in timing/memory\n",
        "    trace_dir=\"/tmp/tpu_module_trace\",\n",
        "    stablehlo_out=\"/tmp/module.stablehlo.mlir\",  # StableHLO export path (set None to skip)\n",
        "    print_report=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Profile any torch.nn.Module on a single TPU core for `iters` iterations AND export StableHLO.\n",
        "\n",
        "    Args:\n",
        "      module_fn: () -> nn.Module\n",
        "      input_fn:  (device, dtype) -> (args, kwargs)\n",
        "      iters:     timed iterations\n",
        "      warmup:    warmup steps (inside profiler)\n",
        "      dtype:     'bf16' or 'fp32' for TPU execution\n",
        "      do_backward: include backward() timing/memory\n",
        "      trace_dir: XLA trace output directory\n",
        "      stablehlo_out: file path to write StableHLO MLIR (None to disable)\n",
        "      print_report: print JSON report\n",
        "\n",
        "    Returns:\n",
        "      Dict with timing, memory, throughput, trace info, and StableHLO export path (if any).\n",
        "    \"\"\"\n",
        "    assert callable(module_fn) and callable(input_fn), \"module_fn and input_fn must be callables\"\n",
        "\n",
        "    device = torch_xla.device()\n",
        "    dtype_map = {\"bf16\": torch.bfloat16, \"fp32\": torch.float32}\n",
        "    dt = dtype_map[dtype]\n",
        "\n",
        "    # Build module and inputs for TPU run\n",
        "    module = module_fn().to(device).to(dt)\n",
        "    args_dev, kwargs_dev = input_fn(device, dt)\n",
        "    if not isinstance(args_dev, (tuple, list)):\n",
        "        raise ValueError(\"input_fn must return (args, kwargs) where args is tuple/list and kwargs is dict\")\n",
        "\n",
        "    # -------- StableHLO export (on CPU, float32) --------\n",
        "    stablehlo_path = None\n",
        "    if stablehlo_out:\n",
        "        module_cpu = module_fn()\n",
        "        args_cpu, kwargs_cpu = input_fn(\"cpu\", torch.float32)\n",
        "        try:\n",
        "            pathlib.Path(os.path.dirname(stablehlo_out) or \".\").mkdir(parents=True, exist_ok=True)\n",
        "            stablehlo_path = _export_stablehlo(module_cpu, args_cpu, kwargs_cpu, stablehlo_out)\n",
        "        except Exception as e:\n",
        "            stablehlo_path = f\"EXPORT_FAILED: {type(e).__name__}: {e}\"\n",
        "\n",
        "    # Enable grads if requested\n",
        "    if do_backward:\n",
        "        for p in module.parameters():\n",
        "            p.requires_grad_(True)\n",
        "\n",
        "    # ---- helpers for memory & throughput ----\n",
        "    def _compute_weight_bytes(mod: torch.nn.Module) -> int:\n",
        "        param_bytes = sum(p.numel() * p.element_size() for p in mod.parameters())\n",
        "        buffer_bytes = sum(b.numel() * b.element_size() for b in mod.buffers())\n",
        "        return param_bytes + buffer_bytes\n",
        "\n",
        "    def _infer_throughput_dims(args, kwargs):\n",
        "        \"\"\"Heuristic: try to detect batch size and 'tokens' / pixels per sample.\"\"\"\n",
        "        tensors = []\n",
        "\n",
        "        def _collect(obj):\n",
        "            if isinstance(obj, torch.Tensor):\n",
        "                tensors.append(obj)\n",
        "            elif isinstance(obj, (list, tuple)):\n",
        "                for x in obj:\n",
        "                    _collect(x)\n",
        "            elif isinstance(obj, dict):\n",
        "                for v in obj.values():\n",
        "                    _collect(v)\n",
        "\n",
        "        _collect(args)\n",
        "        _collect(kwargs)\n",
        "        if not tensors:\n",
        "            return None  # can't infer\n",
        "\n",
        "        # Prefer 3D (B, S, D) -> sequence, then 4D -> image, then 2D -> (B, features)\n",
        "        cand_3d = [t for t in tensors if t.ndim == 3]\n",
        "        cand_4d = [t for t in tensors if t.ndim == 4]\n",
        "        cand_2d = [t for t in tensors if t.ndim == 2]\n",
        "\n",
        "        mode = None\n",
        "        main = None\n",
        "        if cand_3d:\n",
        "            main = max(cand_3d, key=lambda t: t.numel())\n",
        "            B, S = int(main.shape[0]), int(main.shape[1])\n",
        "            tokens_per_sample = S\n",
        "            mode = \"sequence_3d\"\n",
        "        elif cand_4d:\n",
        "            main = max(cand_4d, key=lambda t: t.numel())\n",
        "            B = int(main.shape[0])\n",
        "            # Treat tokens as H*W regardless of channel layout\n",
        "            H, W = int(main.shape[-2]), int(main.shape[-1])\n",
        "            tokens_per_sample = H * W\n",
        "            mode = \"image_4d\"\n",
        "        elif cand_2d:\n",
        "            main = max(cand_2d, key=lambda t: t.numel())\n",
        "            B, S = int(main.shape[0]), int(main.shape[1])\n",
        "            tokens_per_sample = S\n",
        "            mode = \"sequence_2d\"\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        tokens_per_step = B * tokens_per_sample\n",
        "        return {\n",
        "            \"batch_size\": B,\n",
        "            \"tokens_per_sample\": tokens_per_sample,\n",
        "            \"tokens_per_step\": tokens_per_step,\n",
        "            \"mode\": mode,\n",
        "            \"example_shape\": tuple(int(x) for x in main.shape),\n",
        "        }\n",
        "\n",
        "    # Step function\n",
        "    def _one_step():\n",
        "        with torch.autocast(device_type=\"xla\", dtype=dt, enabled=(dtype != \"fp32\")):\n",
        "            out = module(*args_dev, **kwargs_dev)\n",
        "            loss_src = out[0] if isinstance(out, (tuple, list)) else out\n",
        "            if do_backward:\n",
        "                loss = loss_src.float().mean()\n",
        "                loss.backward()\n",
        "                for p in module.parameters():\n",
        "                    if p.grad is not None:\n",
        "                        p.grad.zero_()\n",
        "\n",
        "    # ---- profiling ----\n",
        "    all_warmups = []\n",
        "    times_ms = []\n",
        "    mem_snapshots = []\n",
        "    peak_used_bytes = 0\n",
        "    min_free_bytes = None\n",
        "    total_bytes = None\n",
        "\n",
        "    pathlib.Path(trace_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.XPU],\n",
        "                 record_shapes=True,\n",
        "                 profile_memory=True) as prof:\n",
        "\n",
        "        # Warmup iterations (inside profiler, but reported separately)\n",
        "        for _ in range(max(0, warmup)):\n",
        "            warmup_start = time.perf_counter()\n",
        "            _one_step()\n",
        "            torch_xla.sync()\n",
        "            warmup_end = time.perf_counter()\n",
        "            warmup_ms = (warmup_end - warmup_start) * 1000.0\n",
        "            all_warmups.append(warmup_ms)\n",
        "\n",
        "        # Timed iterations\n",
        "        for i in range(iters):\n",
        "            t0 = time.perf_counter()\n",
        "            _one_step()\n",
        "            torch_xla.sync()\n",
        "            t1 = time.perf_counter()\n",
        "\n",
        "            iter_ms = (t1 - t0) * 1000.0\n",
        "            times_ms.append(iter_ms)\n",
        "\n",
        "            mem = xm.get_memory_info(device)  # {'bytes_used': '...', 'bytes_limit': '...'}\n",
        "            used_bytes = int(mem[\"bytes_used\"])\n",
        "            total_bytes = int(mem[\"bytes_limit\"])\n",
        "            free_bytes = total_bytes - used_bytes\n",
        "\n",
        "            mem_snapshots.append({\n",
        "                \"iter\": i,\n",
        "                \"bytes_used\": used_bytes,\n",
        "                \"bytes_free\": free_bytes,\n",
        "                \"bytes_limit\": total_bytes,\n",
        "            })\n",
        "\n",
        "            if min_free_bytes is None or free_bytes < min_free_bytes:\n",
        "                min_free_bytes = free_bytes\n",
        "            if used_bytes > peak_used_bytes:\n",
        "                peak_used_bytes = used_bytes\n",
        "\n",
        "            prof.step()  # advance profiler step if you later add schedules\n",
        "\n",
        "    # Optional: visualize StableHLO graph\n",
        "    visualize_mlir(stablehlo_out + \"/functions/forward.mlir\", \"\", stablehlo_out + \"/graph_rep\")\n",
        "\n",
        "    # XLA runtime metrics\n",
        "    xla_metrics_text = xmetrics.metrics_report()\n",
        "\n",
        "    # ---- timing summary ----\n",
        "    timing_summary = _summarize_times(times_ms)\n",
        "    timing_summary[\"per_iter_latency_ms\"] = times_ms\n",
        "    timing_summary[\"warmup_latency_ms\"] = all_warmups\n",
        "\n",
        "    # ---- memory summary ----\n",
        "    weight_bytes = _compute_weight_bytes(module)\n",
        "    hbm_total_bytes = total_bytes or 0\n",
        "    activation_est_bytes = max(0, peak_used_bytes - weight_bytes) if hbm_total_bytes > 0 else None\n",
        "    usage_fraction = (peak_used_bytes / hbm_total_bytes) if hbm_total_bytes else None\n",
        "\n",
        "    memory_summary = {\n",
        "        # Raw numbers\n",
        "        \"hbm_total_bytes\": hbm_total_bytes,\n",
        "        \"hbm_total_human\": _human_bytes(hbm_total_bytes) if hbm_total_bytes else None,\n",
        "        \"peak_hbm_used_bytes\": peak_used_bytes,\n",
        "        \"peak_hbm_used_human\": _human_bytes(peak_used_bytes) if peak_used_bytes else None,\n",
        "        \"weights_bytes\": weight_bytes,\n",
        "        \"weights_human\": _human_bytes(weight_bytes),\n",
        "        \"activation_est_bytes\": activation_est_bytes,\n",
        "        \"activation_est_human\": _human_bytes(activation_est_bytes) if activation_est_bytes is not None else None,\n",
        "        \"min_free_bytes\": min_free_bytes,\n",
        "        \"min_free_human\": _human_bytes(min_free_bytes) if min_free_bytes is not None else None,\n",
        "        \"usage_fraction\": usage_fraction,   # peak_used / total (0–1)\n",
        "\n",
        "        # NOTE: true fragmentation (allocator internal fragmentation) is not observable\n",
        "        # from this API; we only expose utilization numbers.\n",
        "        \"fragmentation_estimate\": None,\n",
        "\n",
        "        # Per-iter snapshots for debugging\n",
        "        \"per_iter_bytes_used\": [s[\"bytes_used\"] for s in mem_snapshots],\n",
        "        \"per_iter_bytes_free\": [s[\"bytes_free\"] for s in mem_snapshots],\n",
        "    }\n",
        "\n",
        "    # ---- throughput summary ----\n",
        "    throughput_dims = _infer_throughput_dims(args_dev, kwargs_dev)\n",
        "    throughput_summary = None\n",
        "    if throughput_dims is not None and times_ms:\n",
        "        avg_step_ms = timing_summary.get(\"mean_ms\", sum(times_ms) / len(times_ms))\n",
        "        avg_step_s = avg_step_ms / 1000.0\n",
        "\n",
        "        B = throughput_dims[\"batch_size\"]\n",
        "        tokens_per_sample = throughput_dims[\"tokens_per_sample\"]\n",
        "        tokens_per_step = throughput_dims[\"tokens_per_step\"]\n",
        "        mode = throughput_dims[\"mode\"]\n",
        "\n",
        "        imgs_per_s = None\n",
        "        if mode == \"image_4d\":\n",
        "            imgs_per_s = B / avg_step_s\n",
        "\n",
        "        tokens_per_s = tokens_per_step / avg_step_s\n",
        "\n",
        "        throughput_summary = {\n",
        "            \"mode\": mode,\n",
        "            \"batch_size\": B,\n",
        "            \"tokens_per_sample\": tokens_per_sample,\n",
        "            \"tokens_per_step\": tokens_per_step,\n",
        "            \"avg_step_ms\": avg_step_ms,\n",
        "            \"images_per_second\": imgs_per_s,\n",
        "            \"tokens_per_second\": tokens_per_s,\n",
        "            \"example_input_shape\": throughput_dims[\"example_shape\"],\n",
        "        }\n",
        "\n",
        "    report = {\n",
        "        \"device\": str(device),\n",
        "        \"config\": {\n",
        "            \"iters\": iters,\n",
        "            \"warmup\": warmup,\n",
        "            \"dtype\": dtype,\n",
        "            \"backward\": do_backward,\n",
        "            \"trace_dir\": trace_dir,\n",
        "        },\n",
        "        \"timing_ms\": timing_summary,\n",
        "        \"memory\": memory_summary,\n",
        "        \"throughput\": throughput_summary,\n",
        "        \"trace_dir\": trace_dir,\n",
        "        \"stablehlo_mlir\": stablehlo_path,  # path or EXPORT_FAILED:...\n",
        "        \"xla_metrics_sample\": xla_metrics_text[:2000] + (\"...\" if len(xla_metrics_text) > 2000 else \"\"),\n",
        "        \"trace_howto\": (\n",
        "            f\"Trace written to: {trace_dir}\\n\"\n",
        "            \"In Colab:\\n\"\n",
        "            \"  %load_ext tensorboard\\n\"\n",
        "            f\"  %tensorboard --logdir {trace_dir}\\n\"\n",
        "            \"Open the Profile tab for TPU traces.\"\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    if print_report:\n",
        "        print(json.dumps(report, indent=2))\n",
        "    return report"
      ],
      "metadata": {
        "id": "qjb5w-KZ5mk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ERIC TEMP\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Wrapper (Still needed to sanitize outputs)\n",
        "# ---------------------------------------------------------\n",
        "class GPT2Wrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # use_cache=False is critical for static shapes\n",
        "        out = self.model(input_ids, attention_mask=attention_mask, use_cache=False)\n",
        "        return out.logits\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. StableHLO Export Function\n",
        "# ---------------------------------------------------------\n",
        "def export_stablehlo(module, args, kwargs, output_path):\n",
        "    \"\"\"Export a module to StableHLO MLIR format.\"\"\"\n",
        "    try:\n",
        "        # Option 1: Using torch_xla.stablehlo (if available)\n",
        "        try:\n",
        "            import torch_xla.stablehlo as xla_stablehlo\n",
        "\n",
        "            # Export to StableHLO\n",
        "            stablehlo_program = torch.export.export(module, args, kwargs=kwargs)\n",
        "            stablehlo_module = xla_stablehlo.exported_program_to_stablehlo(stablehlo_program)\n",
        "\n",
        "            # Get the MLIR text\n",
        "            mlir_text = stablehlo_module.get_stablehlo_text()\n",
        "\n",
        "            # Save it\n",
        "            pathlib.Path(os.path.dirname(output_path) or \".\").mkdir(parents=True, exist_ok=True)\n",
        "            with open(output_path, \"w\") as f:\n",
        "                f.write(mlir_text)\n",
        "\n",
        "            print(f\"SUCCESS: StableHLO MLIR saved to {output_path}\")\n",
        "            return output_path\n",
        "\n",
        "        except (ImportError, AttributeError):\n",
        "            # Option 2: Using torch.export + dynamo backend\n",
        "            print(\"torch_xla.stablehlo not available, trying alternative export method...\")\n",
        "\n",
        "            # Export using torch.export API\n",
        "            exported_program = torch.export.export(module, args, kwargs=kwargs)\n",
        "\n",
        "            # Convert to StableHLO using XLA backend\n",
        "            # This requires torch_xla integration\n",
        "            stablehlo_gm = torch_xla.experimental.stablehlo.exported_program_to_stablehlo(\n",
        "                exported_program\n",
        "            )\n",
        "\n",
        "            # Save the MLIR\n",
        "            with open(output_path, \"w\") as f:\n",
        "                f.write(str(stablehlo_gm))\n",
        "\n",
        "            print(f\"SUCCESS: StableHLO MLIR saved to {output_path}\")\n",
        "            return output_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: StableHLO export failed: {e}\")\n",
        "        print(f\"Exception type: {type(e).__name__}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return f\"EXPORT_FAILED: {e}\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Profiler with StableHLO Export\n",
        "# ---------------------------------------------------------\n",
        "def profile_module_on_tpu(\n",
        "    module_fn,\n",
        "    input_fn,\n",
        "    *,\n",
        "    iters=10,\n",
        "    warmup=5,\n",
        "    stablehlo_out=\"gpt2_stablehlo.mlir\",\n",
        "):\n",
        "    device = torch_xla.device()\n",
        "\n",
        "    # Setup TPU module\n",
        "    module_tpu = module_fn().to(device)\n",
        "    args_dev, kwargs_dev = input_fn(device)\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    # A. StableHLO Export Phase (on CPU with tracing)\n",
        "    # -------------------------------------------------------\n",
        "    mlir_path = None\n",
        "    if stablehlo_out:\n",
        "        print(\"\\n=== Exporting to StableHLO ===\")\n",
        "        try:\n",
        "            # Create CPU version for export (StableHLO export works better on CPU)\n",
        "            module_cpu = module_fn()\n",
        "            args_cpu, kwargs_cpu = input_fn(\"cpu\")\n",
        "\n",
        "            # Export to StableHLO\n",
        "            mlir_path = export_stablehlo(module_cpu, args_cpu, kwargs_cpu, stablehlo_out)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: StableHLO export failed: {e}\")\n",
        "            mlir_path = f\"EXPORT_FAILED: {e}\"\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    # B. Profiling Phase (on TPU)\n",
        "    # -------------------------------------------------------\n",
        "    print(\"\\n=== Profiling on TPU ===\")\n",
        "\n",
        "    def _one_step():\n",
        "        with torch.no_grad():\n",
        "            module_tpu(*args_dev, **kwargs_dev)\n",
        "\n",
        "    # Warmup\n",
        "    print(f\"Warming up for {warmup} iterations...\")\n",
        "    for _ in range(warmup):\n",
        "        _one_step()\n",
        "        torch_xla.sync()\n",
        "\n",
        "    # Timing\n",
        "    print(f\"Running {iters} timed iterations...\")\n",
        "    times = []\n",
        "    for i in range(iters):\n",
        "        t0 = time.perf_counter()\n",
        "        _one_step()\n",
        "        torch_xla.sync()\n",
        "        t1 = time.perf_counter()\n",
        "        times.append((t1 - t0) * 1000)\n",
        "\n",
        "    avg_ms = sum(times) / len(times)\n",
        "    min_ms = min(times)\n",
        "    max_ms = max(times)\n",
        "\n",
        "    print(f\"\\nLatency Statistics:\")\n",
        "    print(f\"  Average: {avg_ms:.2f} ms\")\n",
        "    print(f\"  Min:     {min_ms:.2f} ms\")\n",
        "    print(f\"  Max:     {max_ms:.2f} ms\")\n",
        "\n",
        "    return {\n",
        "        \"mlir_path\": mlir_path,\n",
        "        \"latency_ms\": {\n",
        "            \"avg\": avg_ms,\n",
        "            \"min\": min_ms,\n",
        "            \"max\": max_ms,\n",
        "            \"all\": times\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Run It\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def gpt2_module_fn():\n",
        "    base = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "    return GPT2Wrapper(base)\n",
        "\n",
        "def gpt2_input_fn(device):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "    enc = tokenizer(\n",
        "        [text] * 2,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=64,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    if device == \"cpu\":\n",
        "        return (enc[\"input_ids\"], enc[\"attention_mask\"]), {}\n",
        "    else:\n",
        "        return (enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device)), {}\n",
        "\n",
        "print(\"\\n================ NLP: GPT-2 small (StableHLO Export) ================\")\n",
        "result = profile_module_on_tpu(\n",
        "    module_fn=gpt2_module_fn,\n",
        "    input_fn=gpt2_input_fn,\n",
        "    stablehlo_out=\"gpt2_stablehlo.mlir\"\n",
        ")\n",
        "\n",
        "print(f\"\\nResults: {json.dumps(result, indent=2)}\")"
      ],
      "metadata": {
        "id": "3YE99xZG4nET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from Gemimi to get stable HLO for GPT-2 small:\n",
        "\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import time\n",
        "import os\n",
        "import pathlib\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Wrapper (Essential for clean graph capture)\n",
        "# ---------------------------------------------------------\n",
        "class GPT2Wrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Disable cache to ensure a static graph (no dynamic shapes)\n",
        "        out = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            use_cache=False,\n",
        "            return_dict=True\n",
        "        )\n",
        "        return out.logits\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Profiler with StableHLO Runtime Capture\n",
        "# ---------------------------------------------------------\n",
        "def profile_module_on_tpu(\n",
        "    module_fn,\n",
        "    input_fn,\n",
        "    *,\n",
        "    iters=5,\n",
        "    warmup=2,\n",
        "    stablehlo_out=\"gpt2_stablehlo.mlir\",\n",
        "):\n",
        "    device = xm.xla_device()\n",
        "\n",
        "    # Instantiate model on TPU\n",
        "    module = module_fn().to(device)\n",
        "    args_dev, kwargs_dev = input_fn(device)\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    # A. Capture StableHLO directly from Runtime\n",
        "    # -------------------------------------------------------\n",
        "    mlir_path = None\n",
        "    if stablehlo_out:\n",
        "        print(\"Attempting Runtime StableHLO Capture...\")\n",
        "        try:\n",
        "            # 1. Run Forward Pass (Lazy Tensors created)\n",
        "            output = module(*args_dev, **kwargs_dev)\n",
        "\n",
        "            # 2. Capture Graph\n",
        "            # Try the modern StableHLO API first\n",
        "            if hasattr(xm, \"get_stablehlo\"):\n",
        "                # This returns the graph in StableHLO dialect\n",
        "                graph_text = xm.get_stablehlo([output])\n",
        "            else:\n",
        "                # Fallback for older torch_xla versions (returns HLO, similar but not identical)\n",
        "                print(\"WARNING: xm.get_stablehlo not found. Falling back to HLO capture.\")\n",
        "                graph_text = torch_xla._XLAC._get_xla_tensors_text([output])\n",
        "\n",
        "            # 3. Save\n",
        "            pathlib.Path(os.path.dirname(stablehlo_out) or \".\").mkdir(parents=True, exist_ok=True)\n",
        "            with open(stablehlo_out, \"w\") as f:\n",
        "                f.write(graph_text)\n",
        "\n",
        "            mlir_path = stablehlo_out\n",
        "            print(f\"SUCCESS: IR saved to {stablehlo_out}\")\n",
        "\n",
        "            # Clear graph to prevent memory accumulation\n",
        "            xm.mark_step()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Capture failed: {e}\")\n",
        "            mlir_path = f\"CAPTURE_FAILED: {e}\"\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    # B. Performance Profiling\n",
        "    # -------------------------------------------------------\n",
        "    def _one_step():\n",
        "        with torch.no_grad():\n",
        "            module(*args_dev, **kwargs_dev)\n",
        "\n",
        "    print(f\"Running {iters} iterations...\")\n",
        "    # Warmup\n",
        "    for _ in range(warmup):\n",
        "        _one_step()\n",
        "        xm.mark_step()\n",
        "\n",
        "    # Timing\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(iters):\n",
        "        _one_step()\n",
        "        xm.mark_step()\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    avg_ms = ((t1 - t0) * 1000) / iters\n",
        "    print(f\"Avg Latency: {avg_ms:.2f} ms\")\n",
        "\n",
        "    return {\"mlir_path\": mlir_path, \"latency_ms\": avg_ms}\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Execution\n",
        "# ---------------------------------------------------------\n",
        "def gpt2_module_fn():\n",
        "    base = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "    return GPT2Wrapper(base)\n",
        "\n",
        "def gpt2_input_fn(device):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "    text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "    enc = tokenizer(\n",
        "        [text] * 2,\n",
        "        padding=\"max_length\", truncation=True, max_length=64,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return (enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device)), {}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    profile_module_on_tpu(\n",
        "        module_fn=gpt2_module_fn,\n",
        "        input_fn=gpt2_input_fn,\n",
        "        stablehlo_out=\"gpt2_stablehlo.mlir\"\n",
        "    )"
      ],
      "metadata": {
        "id": "p6gkhjzg40mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED, HEADS = 256, 16\n",
        "BATCH, SEQ = 128, 64\n",
        "\n",
        "# MHA\n",
        "\n",
        "profile_module_on_tpu(\n",
        "    module_fn=mha_module_fn(embed_dim=EMBED, num_heads=HEADS, batch_first=True),\n",
        "    input_fn=mha_input_fn(batch=BATCH, seq_len=SEQ, embed_dim=EMBED),\n",
        "    iters=10,\n",
        "    warmup=5,\n",
        "    dtype=\"bf16\",                     # 'bf16' recommended on TPU\n",
        "    do_backward=False,\n",
        "    trace_dir=\"tpu_mha_trace_generic\",\n",
        "    stablehlo_out=\"mha.stablehlo.mlir\",  # <-- StableHLO export file\n",
        "    print_report=True\n",
        ")"
      ],
      "metadata": {
        "id": "gHzl-UMP5pjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED, HEADS = 256, 16\n",
        "BATCH, SEQ = 128, 64\n",
        "\n",
        "# GEMM\n",
        "\n",
        "profile_module_on_tpu(\n",
        "    module_fn=gemm_module_fn(),\n",
        "    input_fn=gemm_input_fn(),\n",
        "    iters=10,\n",
        "    warmup=5,\n",
        "    dtype=\"bf16\",                     # 'bf16' recommended on TPU\n",
        "    do_backward=False,\n",
        "    trace_dir=\"tpu_gemm_trace_generic\",\n",
        "    stablehlo_out=\"gemm.stablehlo.mlir\",  # <-- StableHLO export file\n",
        "    print_report=True\n",
        ")"
      ],
      "metadata": {
        "id": "oerbES46-LD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED, HEADS = 256, 16\n",
        "BATCH, SEQ = 128, 64\n",
        "\n",
        "# FFN\n",
        "\n",
        "profile_module_on_tpu(\n",
        "    module_fn=ffn_module_fn(),\n",
        "    input_fn=ffn_input_fn(),\n",
        "    iters=10,\n",
        "    warmup=5,\n",
        "    dtype=\"bf16\",                     # 'bf16' recommended on TPU\n",
        "    do_backward=False,\n",
        "    trace_dir=\"tpu_ffn_trace_generic\",\n",
        "    stablehlo_out=\"ffn.stablehlo.mlir\",  # <-- StableHLO export file\n",
        "    print_report=True\n",
        ")"
      ],
      "metadata": {
        "id": "tZwmKKmy_ZnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MtNuyFdkp_V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.runtime as xr\n",
        "\n",
        "def clear_tpu_state():\n",
        "    try:\n",
        "        xm.mark_step()\n",
        "        xm.wait_device_ops()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Clear compile cache (supported on v5e)\n",
        "    try:\n",
        "        xr.clear_all_cache()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    print(\"✔️ Cleared TPU runtime & compile cache\")"
      ],
      "metadata": {
        "id": "WTg-FG-7p82n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bfHZh6ykp8qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 1) Vision: ResNet-50\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def resnet50_module_fn(model_name=\"microsoft/resnet-50\"):\n",
        "    def _build():\n",
        "        return AutoModelForImageClassification.from_pretrained(model_name)\n",
        "    return _build\n",
        "\n",
        "def resnet50_input_fn(batch=8, height=224, width=224):\n",
        "    def _make(device, dt):\n",
        "        # HF vision models expect NCHW\n",
        "        x = torch.randn(batch, 3, height, width, device=device, dtype=dt)\n",
        "        return (x,), {}\n",
        "    return _make\n",
        "\n",
        "print(\"\\n================ Vision: ResNet-50 (PyTorch) ================\")\n",
        "profile_module_on_tpu(\n",
        "    module_fn=resnet50_module_fn(),\n",
        "    input_fn=resnet50_input_fn(batch=8, height=224, width=224),\n",
        "    iters=10,\n",
        "    warmup=5,\n",
        "    dtype=\"bf16\",\n",
        "    do_backward=False,\n",
        "    trace_dir=\"tpu_resnet50_trace\",\n",
        "    stablehlo_out=\"resnet50.stablehlo.mlir\",\n",
        "    print_report=True,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "9Rz0Y59PxQck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2) Vision: ViT-B/16\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def vit_module_fn(model_name=\"google/vit-base-patch16-224\"):\n",
        "    def _build():\n",
        "        return AutoModelForImageClassification.from_pretrained(model_name)\n",
        "    return _build\n",
        "\n",
        "def vit_input_fn(batch=8, height=224, width=224):\n",
        "    def _make(device, dt):\n",
        "        # Vit image models also expect NCHW\n",
        "        x = torch.randn(batch, 3, height, width, device=device, dtype=dt)\n",
        "        return (x,), {}\n",
        "    return _make\n",
        "\n",
        "print(\"\\n================ Vision: ViT-B/16 (PyTorch) ================\")\n",
        "profile_module_on_tpu(\n",
        "    module_fn=vit_module_fn(),\n",
        "    input_fn=vit_input_fn(batch=8, height=224, width=224),\n",
        "    iters=10,\n",
        "    warmup=5,\n",
        "    dtype=\"bf16\",\n",
        "    do_backward=False,\n",
        "    trace_dir=\"tpu_vit_b16_trace\",\n",
        "    stablehlo_out=\"vit_b16.stablehlo.mlir\",\n",
        "    print_report=True,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "2jIPCfUwzlFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3) NLP: BERT-base (sequence classification)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def bert_base_module_fn(model_name=\"bert-base-uncased\"):\n",
        "    def _build():\n",
        "        return AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    return _build\n",
        "\n",
        "def bert_base_input_fn(\n",
        "    model_name=\"bert-base-uncased\",\n",
        "    batch=4,\n",
        "    seq_len=128,\n",
        "):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def _make(device, dt):\n",
        "        # Tokens are integer IDs – keep them as long, ignore dt\n",
        "        enc = tokenizer(\n",
        "            [\"this is a dummy sentence\"] * batch,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=seq_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = enc[\"input_ids\"].to(device)\n",
        "        attention_mask = enc[\"attention_mask\"].to(device)\n",
        "        # Call pattern: module(input_ids, attention_mask=...)\n",
        "        return (input_ids,), {\"attention_mask\": attention_mask}\n",
        "\n",
        "    return _make\n",
        "\n",
        "print(\"\\n================ NLP: BERT-base (PyTorch) ================\")\n",
        "profile_module_on_tpu(\n",
        "    module_fn=bert_base_module_fn(),\n",
        "    input_fn=bert_base_input_fn(\n",
        "        model_name=\"bert-base-uncased\",\n",
        "        batch=4,\n",
        "        seq_len=128,\n",
        "    ),\n",
        "    iters=10,\n",
        "    warmup=5,\n",
        "    dtype=\"bf16\",\n",
        "    do_backward=False,\n",
        "    trace_dir=\"tpu_bert_base_trace\",\n",
        "    stablehlo_out=\"bert_base.stablehlo.mlir\",\n",
        "    print_report=True,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "ztp3lg61znI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4) NLP: GPT-2 small\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def gpt2_module_fn(model_name=\"gpt2\"):\n",
        "    def _build():\n",
        "        return AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    return _build\n",
        "\n",
        "def gpt2_input_fn(\n",
        "    model_name=\"gpt2\",\n",
        "    batch=2,\n",
        "    seq_len=64,\n",
        "    vocab_example_text=\"The quick brown fox jumps over the lazy dog.\",\n",
        "):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    # GPT-2 has no pad_token by default – set it for padding\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    def _make(device, dt):\n",
        "        enc = tokenizer(\n",
        "            [vocab_example_text] * batch,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=seq_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = enc[\"input_ids\"].to(device)\n",
        "        attention_mask = enc[\"attention_mask\"].to(device)\n",
        "        # Call pattern: module(input_ids, attention_mask=...)\n",
        "        return (input_ids,), {\"attention_mask\": attention_mask}\n",
        "\n",
        "    return _make\n",
        "\n",
        "print(\"\\n================ NLP: GPT-2 small (PyTorch) ================\")\n",
        "profile_module_on_tpu(\n",
        "    module_fn=gpt2_module_fn(),\n",
        "    input_fn=gpt2_input_fn(\n",
        "        model_name=\"gpt2\",\n",
        "        batch=2,\n",
        "        seq_len=64,\n",
        "        vocab_example_text=\"The quick brown fox jumps over the lazy dog.\",\n",
        "    ),\n",
        "    iters=10,\n",
        "    warmup=5,\n",
        "    dtype=\"bf16\",\n",
        "    do_backward=False,\n",
        "    trace_dir=\"tpu_gpt2_small_trace\",\n",
        "    stablehlo_out=\"gpt2_small.stablehlo.mlir\",\n",
        "    print_report=True,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "AgCAPsMZzo7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5) NLP: GPT-Neo 1.3B\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def gptneo_module_fn(model_name=\"EleutherAI/gpt-neo-1.3B\"):\n",
        "    def _build():\n",
        "        return AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    return _build\n",
        "\n",
        "def gptneo_input_fn(\n",
        "    model_name=\"EleutherAI/gpt-neo-1.3B\",\n",
        "    batch=1,\n",
        "    seq_len=64,\n",
        "    vocab_example_text=\"Dynamic pruning and batching for large language models.\",\n",
        "):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    def _make(device, dt):\n",
        "        enc = tokenizer(\n",
        "            [vocab_example_text] * batch,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=seq_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = enc[\"input_ids\"].to(device)\n",
        "        attention_mask = enc[\"attention_mask\"].to(device)\n",
        "        return (input_ids,), {\"attention_mask\": attention_mask}\n",
        "\n",
        "    return _make\n",
        "\n",
        "print(\"\\n================ NLP: GPT-Neo 1.3B (PyTorch) ================\")\n",
        "profile_module_on_tpu(\n",
        "    module_fn=gptneo_module_fn(),\n",
        "    input_fn=gptneo_input_fn(\n",
        "        model_name=\"EleutherAI/gpt-neo-1.3B\",\n",
        "        batch=1,\n",
        "        seq_len=64,\n",
        "        vocab_example_text=\"Dynamic pruning and batching for large language models.\",\n",
        "    ),\n",
        "    iters=10,\n",
        "    warmup=5,\n",
        "    dtype=\"bf16\",\n",
        "    do_backward=False,\n",
        "    trace_dir=\"tpu_gptneo_1_3b_trace\",\n",
        "    stablehlo_out=\"gptneo_1_3b.stablehlo.mlir\",\n",
        "    print_report=True,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "02aZyOCrzq8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# 6) Diffusion: UNet (Stable Diffusion)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def unet_module_fn(model_name=\"runwayml/stable-diffusion-v1-5\"):\n",
        "    def _build():\n",
        "        # Load the UNet from Stable Diffusion v1.5\n",
        "        return UNet2DConditionModel.from_pretrained(\n",
        "            model_name,\n",
        "            subfolder=\"unet\",\n",
        "        )\n",
        "    return _build\n",
        "\n",
        "def unet_input_fn(\n",
        "    batch=1,\n",
        "    height=64,\n",
        "    width=64,\n",
        "    channels=4,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build dummy inputs matching your JAX profiling:\n",
        "    - sample: [B, 4, 64, 64]\n",
        "    - timesteps: [B]\n",
        "    - encoder_hidden_states: [B, 77, 768]\n",
        "    \"\"\"\n",
        "    def _make(device, dt):\n",
        "        sample = torch.randn(batch, channels, height, width, device=device, dtype=dt)\n",
        "        timesteps = torch.ones(batch, device=device, dtype=torch.float32)\n",
        "        encoder_hidden_states = torch.randn(batch, 77, 768, device=device, dtype=dt)\n",
        "        return (sample, timesteps, encoder_hidden_states), {}\n",
        "\n",
        "    return _make\n",
        "\n",
        "print(\"\\n================ Diffusion: UNet (Stable Diffusion) ================\")\n",
        "profile_module_on_tpu(\n",
        "    module_fn=unet_module_fn(),\n",
        "    input_fn=unet_input_fn(\n",
        "        batch=1,\n",
        "        height=64,\n",
        "        width=64,\n",
        "        channels=4,\n",
        "    ),\n",
        "    iters=10,\n",
        "    warmup=5,\n",
        "    dtype=\"bf16\",\n",
        "    do_backward=False,\n",
        "    trace_dir=\"tpu_unet_sd_trace\",\n",
        "    stablehlo_out=\"unet_sd.stablehlo.mlir\",\n",
        "    print_report=True,\n",
        ")"
      ],
      "metadata": {
        "id": "x3frh3f6ztLa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}